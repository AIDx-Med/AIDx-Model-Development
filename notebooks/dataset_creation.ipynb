{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01fcc3cf7b2b31b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import URL\n",
    "import os\n",
    "\n",
    "HOST_IP = os.environ[\"DATABASE_IP\"]\n",
    "DATABASE_USER = os.environ[\"DATABASE_USER\"]\n",
    "DATABASE_PASSWORD = os.environ[\"DATABASE_PASSWORD\"]\n",
    "DATABASE_PORT = os.environ[\"DATABASE_PORT\"]\n",
    "\n",
    "connection_url = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=DATABASE_USER,\n",
    "    password=DATABASE_PASSWORD,\n",
    "    host=HOST_IP,\n",
    "    port=DATABASE_PORT,\n",
    "    database=\"mimicllm\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41857765567af325",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Text, BigInteger\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class TokenizedData(Base):\n",
    "    __tablename__ = \"tokenized_data\"\n",
    "    __table_args__ = {\"schema\": \"mimicllm\"}\n",
    "\n",
    "    token_id = Column(BigInteger, primary_key=True)\n",
    "    attention_mask = Column(Text)\n",
    "    input_ids = Column(Text)\n",
    "\n",
    "\n",
    "# Replace with your database connection details\n",
    "engine = create_engine(connection_url)\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "chunk_size = 1000\n",
    "test_size = 0.2\n",
    "\n",
    "parquet_dir = \"data\"\n",
    "\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "train_parquet_file = os.path.join(parquet_dir, \"train.parquet\")\n",
    "test_parquet_file = os.path.join(parquet_dir, \"test.parquet\")\n",
    "\n",
    "last_id = 0\n",
    "\n",
    "# Initialize Parquet writers\n",
    "train_writer = None\n",
    "test_writer = None\n",
    "\n",
    "# Usage\n",
    "session = Session()\n",
    "\n",
    "total_rows = session.query(TokenizedData).count()\n",
    "\n",
    "with tqdm(total=total_rows, desc=\"Processing\") as pbar:\n",
    "    while True:\n",
    "        # Query the database in chunks ordered by a unique column\n",
    "        query = (\n",
    "            session.query(TokenizedData)\n",
    "            .order_by(TokenizedData.token_id)\n",
    "            .filter(TokenizedData.token_id > last_id)\n",
    "            .limit(chunk_size)\n",
    "        )\n",
    "        chunk = pd.read_sql(query.statement, session.bind)\n",
    "\n",
    "        if chunk.empty:\n",
    "            break\n",
    "\n",
    "        train_chunk, test_chunk = train_test_split(chunk, test_size=test_size)\n",
    "\n",
    "        # Convert DataFrame to PyArrow Table\n",
    "        train_table = pa.Table.from_pandas(train_chunk, preserve_index=False)\n",
    "        test_table = pa.Table.from_pandas(test_chunk, preserve_index=False)\n",
    "\n",
    "        # Write train chunk\n",
    "        if train_writer is None:\n",
    "            train_writer = pq.ParquetWriter(\n",
    "                train_parquet_file, train_table.schema, compression=\"snappy\"\n",
    "            )\n",
    "        train_writer.write_table(train_table)\n",
    "\n",
    "        # Write test chunk\n",
    "        if test_writer is None:\n",
    "            test_writer = pq.ParquetWriter(\n",
    "                test_parquet_file, test_table.schema, compression=\"snappy\"\n",
    "            )\n",
    "\n",
    "        test_writer.write_table(test_table)\n",
    "\n",
    "        last_id = int(chunk[\"token_id\"].iloc[-1])\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(len(chunk))\n",
    "\n",
    "        break\n",
    "\n",
    "# Close the Parquet writers\n",
    "if train_writer:\n",
    "    train_writer.close()\n",
    "if test_writer:\n",
    "    test_writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9d728fa67874429",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "50bf45127ae6241e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
