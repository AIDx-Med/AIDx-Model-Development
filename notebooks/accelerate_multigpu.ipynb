{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5085fbd8-c1ff-4910-b1b8-c725db2cbecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:128'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from ..src.training.training_utils import *\n",
    "\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf9c36c-3de0-432c-90f1-36e7a22282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parquet_dir = '/workspace/data'\n",
    "    stream_data = False\n",
    "    base_model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "    \n",
    "    cpu_count = os.cpu_count()\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    \n",
    "    print(\"Setting up training environment...\")\n",
    "    accelerator = setup_training_env()\n",
    "    data_collator = create_data_collator(base_model_id)\n",
    "    accelerator.print(\"Done setting up training environment...\")\n",
    "    \n",
    "    accelerator.print(f\"Number of CPUs available: {cpu_count}\")\n",
    "    accelerator.print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    \n",
    "    accelerator.print(\"Loading data...\")\n",
    "    train_data, val_data, train_count, val_count = load_data(parquet_dir, stream=stream_data, cpu_count=cpu_count, accelerator=accelerator)\n",
    "    accelerator.print(\"Done loading data...\")\n",
    "    \n",
    "    accelerator.print(\"Loading compute_bertscore...\")\n",
    "    # compute_bertscore = load_bertscore()\n",
    "    accelerator.print(\"Done loading compute_bertscore...\")\n",
    "\n",
    "    # x = train_data.select(range(100))\n",
    "    \n",
    "    # def test(a):\n",
    "    #     return {\n",
    "    #         'attention_mask': a['attention_mask'][:512],\n",
    "    #         'input_ids': a['input_ids'][:512]\n",
    "    #     }\n",
    "    \n",
    "    # x = x.map(test)\n",
    "    \n",
    "    accelerator.print(\"Loading model trainer...\")\n",
    "    trainer = load_model_trainer(\n",
    "            base_model_id,\n",
    "            False,\n",
    "            data_collator,\n",
    "            train_data,\n",
    "            val_data,\n",
    "            train_count,\n",
    "            cpu_count//gpu_count,\n",
    "            accelerator\n",
    "        )\n",
    "    \n",
    "    print_gpu_utilization(accelerator)\n",
    "\n",
    "    train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98921e84-478f-4fe5-b3e3-df32446e9c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Setting up training environment...\n",
      "Setting up training environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malan-alwakeel\u001B[0m (\u001B[33maidx\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33malan-alwakeel\u001B[0m (\u001B[33maidx\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f8b73513ca4c2f89a743506412adbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77da35af1e3a489ab99aa79fd6dcd2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c54a00649e4325b894a2398a007fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e8cc59306840bf91281ba74424766b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer_config.json\n",
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting up training environment...\n",
      "Number of CPUs available: 48\n",
      "Number of GPUs available: 2\n",
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da9fe616e0c43a59f63e0ea891fae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddca0c6828349c0b8166b8b27681ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/4718244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data...\n",
      "Loading compute_bertscore...\n",
      "Done loading compute_bertscore...\n",
      "Loading model trainer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974cf4387f3445d9954e796238ce5353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.2.0\n",
      "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.2.0\n",
      "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f11009dc3f46c28c6ee1d8a423b95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efffc5a344a0458583996006878c5226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b6dfd396f44417a03dae072c874e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bde61d3fa14b8095829a3335486884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1186abb933479f876fa16428e55bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16527c76bcc04669b8cdebbb18bccc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b9e33bb8fc48738fc56bb53924a8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21143181941b4758b64e768f5c83cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d6189c5d894d55a87f4d568cf1d0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31db0b3b8d947358b21e6d5008343fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34cf6d3e1424014bd7b9fb5cc7515b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8323d799604342ada6592213ed19b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62c1995750e4205b188a276431a06a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c076e132834a4382c2a597a0d3a603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a9e1016b0a4dd4ba1ccebaf491882b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd29f12197542b48611292b7933f71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c3915af4e047868223c247987bb664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce24f1de8394dbcaa053aec6f7c187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e79899b8d04cae9811639eabd02c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(main, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976931ee-c79a-4283-b723-9f3c1086915a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
