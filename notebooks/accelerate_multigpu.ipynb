{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5085fbd8-c1ff-4910-b1b8-c725db2cbecf",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T15:44:32.123430Z",
     "start_time": "2024-02-03T15:44:27.474008Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:128'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "current_file_directory = os.path.dirname(os.path.abspath(''))\n",
    "absolute_src_path = os.path.abspath(current_file_directory)\n",
    "sys.path.append(absolute_src_path)\n",
    "\n",
    "from src.training.training_utils import *\n",
    "\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94c9e68-c143-46a1-ad84-260565ae6a9a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T15:45:15.162424Z",
     "start_time": "2024-02-03T15:44:32.125870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.1.0\n",
      "Detected flash_attn version 2.5.2\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/model.safetensors.index.json\n",
      "Detected flash_attn version 2.5.2\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Detected flash_attn version 2.5.2\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Detected flash_attn version 2.5.2\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes. No dtype was provided, you should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator.\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n",
      "Detected flash_attn version 2.5.2\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e62797a921a84e84bf6a7632ccd1ec15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MixtralForCausalLM.\n",
      "\n",
      "All the weights of MixtralForCausalLM were initialized from the model checkpoint at mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MixtralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mixtral-8x7B-Instruct-v0.1\", attn_implementation=\"flash_attention_2\",\n",
    "       # device_map='auto', #quantization_config=bnb_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2508de-b3bb-4efb-9489-937be7d32603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T15:48:04.966498Z",
     "start_time": "2024-02-03T15:47:58.627059Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=8,\n",
    "        target_modules=[\n",
    "            \"gate\",\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"w1\",\n",
    "            \"w2\",\n",
    "            \"w3\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "lora_model = PeftModelForCausalLM.from_pretrained(model, '/workspace/best-model', config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MixtralForCausalLM(\n      (model): MixtralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MixtralDecoderLayer(\n            (self_attn): MixtralFlashAttention2(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MixtralRotaryEmbedding()\n            )\n            (block_sparse_moe): MixtralSparseMoeBlock(\n              (gate): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=8, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (experts): ModuleList(\n                (0-7): 8 x MixtralBLockSparseTop2MLP(\n                  (w1): lora.Linear(\n                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=14336, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (w2): lora.Linear(\n                    (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=14336, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=4096, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (w3): lora.Linear(\n                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=14336, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n            )\n            (input_layernorm): MixtralRMSNorm()\n            (post_attention_layernorm): MixtralRMSNorm()\n          )\n        )\n        (norm): MixtralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.requires_grad_(False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T15:49:02.635919Z",
     "start_time": "2024-02-03T15:49:02.594929Z"
    }
   },
   "id": "afcebe456a183efa",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MixtralForCausalLM(\n      (model): MixtralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MixtralDecoderLayer(\n            (self_attn): MixtralFlashAttention2(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MixtralRotaryEmbedding()\n            )\n            (block_sparse_moe): MixtralSparseMoeBlock(\n              (gate): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=8, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (experts): ModuleList(\n                (0-7): 8 x MixtralBLockSparseTop2MLP(\n                  (w1): lora.Linear(\n                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=14336, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (w2): lora.Linear(\n                    (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=14336, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=4096, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (w3): lora.Linear(\n                    (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=14336, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n            )\n            (input_layernorm): MixtralRMSNorm()\n            (post_attention_layernorm): MixtralRMSNorm()\n          )\n        )\n        (norm): MixtralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.to(torch.bfloat16).to('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T15:50:53.391117Z",
     "start_time": "2024-02-03T15:50:36.967948Z"
    }
   },
   "id": "7fffd0d3747117c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m merged_model \u001B[38;5;241m=\u001B[39m \u001B[43mlora_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge_and_unload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/peft/tuners/lora/model.py:620\u001B[0m, in \u001B[0;36mLoraModel.merge_and_unload\u001B[0;34m(self, progressbar, safe_merge, adapter_names)\u001B[0m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmerge_and_unload\u001B[39m(\n\u001B[1;32m    593\u001B[0m     \u001B[38;5;28mself\u001B[39m, progressbar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, safe_merge: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, adapter_names: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    594\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule:\n\u001B[1;32m    595\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;124;03m    This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03m    as a standalone model.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    619\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_unload_and_optionally_merge\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogressbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogressbar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_merge\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe_merge\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madapter_names\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/peft/tuners/lora/model.py:352\u001B[0m, in \u001B[0;36mLoraModel._unload_and_optionally_merge\u001B[0;34m(self, merge, progressbar, safe_merge, adapter_names)\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(target, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_layer\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    351\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m merge:\n\u001B[0;32m--> 352\u001B[0m         \u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43msafe_merge\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe_merge\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madapter_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replace_module(parent, target_name, target\u001B[38;5;241m.\u001B[39mget_base_layer(), target)\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(target, ModulesToSaveWrapper):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;66;03m# save any additional trainable modules part of `modules_to_save`\u001B[39;00m\n",
      "File \u001B[0;32m/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/peft/tuners/lora/layer.py:248\u001B[0m, in \u001B[0;36mLinear.merge\u001B[0;34m(self, safe_merge, adapter_names)\u001B[0m\n\u001B[1;32m    246\u001B[0m     base_layer\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m orig_weights\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 248\u001B[0m     base_layer\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_delta_weight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactive_adapter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerged_adapters\u001B[38;5;241m.\u001B[39mappend(active_adapter)\n",
      "File \u001B[0;32m/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/peft/tuners/lora/layer.py:286\u001B[0m, in \u001B[0;36mLinear.get_delta_weight\u001B[0;34m(self, adapter)\u001B[0m\n\u001B[1;32m    283\u001B[0m     weight_A \u001B[38;5;241m=\u001B[39m weight_A\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m    284\u001B[0m     weight_B \u001B[38;5;241m=\u001B[39m weight_B\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m--> 286\u001B[0m output_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight_B\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mweight_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfan_in_fan_out\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaling[adapter]\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cast_to_fp32:\n\u001B[1;32m    289\u001B[0m     output_tensor \u001B[38;5;241m=\u001B[39m output_tensor\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "File \u001B[0;32m/workspace/conda/envs/aidx-model-development/lib/python3.10/site-packages/peft/utils/other.py:396\u001B[0m, in \u001B[0;36mtranspose\u001B[0;34m(weight, fan_in_fan_out)\u001B[0m\n\u001B[1;32m    392\u001B[0m     auto_wrap_policy \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(_or_policy, policies\u001B[38;5;241m=\u001B[39m[lambda_policy, transformer_wrap_policy])\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m auto_wrap_policy\n\u001B[0;32m--> 396\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranspose\u001B[39m(weight, fan_in_fan_out):\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fan_in_fan_out:\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m weight\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "merged_model = lora_model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T16:12:27.769624Z",
     "start_time": "2024-02-03T15:51:22.940171Z"
    }
   },
   "id": "d01723d96e4a282",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_model.save_pretrained('/workspace/aidx-copilot')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T16:12:27.770873Z"
    }
   },
   "id": "1deb157347ffeb48"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # restore padding side\n",
    "tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n",
    "\n",
    "tokenizer.save_pretrained('/workspace/aidx-copilot')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T16:12:27.772659Z",
     "start_time": "2024-02-03T16:12:27.770990Z"
    }
   },
   "id": "2adabadf65747a19"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c35912-f8e1-49f1-8c60-f0d62a83de4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:55:41.565391Z",
     "start_time": "2024-02-03T02:55:41.236121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-Instruct-v0.1/snapshots/125c431e2ff41a156b9f9076f744d2f35dd6e67a/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # restore padding side\n",
    "tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n",
    "\n",
    "# generator = pipeline(\"conversational\", model=lora_model, tokenizer=tokenizer)\n",
    "generator = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f828c81-75b1-4c0d-82f9-bd106a05c882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:55:41.573234Z",
     "start_time": "2024-02-03T02:55:41.572567Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1, tokenizer=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.ENCOUNTERS = encounters\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        assert tokenizer is not None, \"Tokenizer is required\"\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        stop_count = 0\n",
    "        for input_ids_list in input_ids:\n",
    "            for stop in self.stops:\n",
    "                length = len(stop) + 5 # buffer for special tokens preceeding stop\n",
    "                \n",
    "                if len(input_ids_list) < length:\n",
    "                    continue\n",
    "\n",
    "                last_elements = input_ids_list[-length:]\n",
    "                decoded_elements = self.tokenizer.decode(last_elements)\n",
    "\n",
    "                if stop in decoded_elements:\n",
    "                    stop_count += 1\n",
    "\n",
    "        if stop_count >= self.ENCOUNTERS:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words = [\"<|im_end|>\"]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words, tokenizer=tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16b5643-f685-4cb4-a628-443d7b6df40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:55:41.579978Z",
     "start_time": "2024-02-03T02:55:41.574591Z"
    }
   },
   "outputs": [],
   "source": [
    "system_msg = 'You are AIDx, a specialized Large Language Model designed to assist medical professionals in clinical decision-making. Your primary task is to analyze patient data and provide comprehensive, medically sound recommendations. Each output you provide should encompass detailed suggestions for potential diseases, personalized treatment options, and recommended future actions. It is imperative that your advice rigorously aligns with the latest medical standards, emphasizing accuracy, relevance, and patient safety. Your role is crucial in supporting healthcare professionals, enabling them to make well-informed, evidence-based decisions regarding patient care. When responding to inquiries, it is essential to demonstrate a clear and thorough thought process. Begin by methodically analyzing and interpreting each symptom, sign, and lab finding presented in the query. Use this analysis to inform and structure your suggestions, ensuring each reasoning step is explicitly articulated and medically justified. This approach will foster deeper understanding and trust in your assessments and recommendations among medical professionals. Due to regulatory restrictions, avoid using the word diagnosis, instead use a phrase relating to predicting disease. Given that your target user is a medical professional, include a phrase emphasizing that the advice is AI-generated and should be followed with discretion and caution, reminding users of the inherent limitations of AI in medical decision-making.'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConversationChain:\n",
    "    def __init__(self, system_prompt='', conversation=[], generator=None, stopping_criteria=[], max_length=32_000):\n",
    "        self.system_msg = {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        }\n",
    "                \n",
    "        self.conversation = [self.system_msg] + conversation\n",
    "        \n",
    "        assert generator is not None, \"Generator (pipeline) is required!\"\n",
    "        self.generator = generator\n",
    "        \n",
    "        self.stopping_criteria = stopping_criteria\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, message):\n",
    "        new_conversation = self.conversation + [{\n",
    "            'role': 'user',\n",
    "            'content': message\n",
    "        }]\n",
    "        \n",
    "        return self._predict(new_conversation)\n",
    "\n",
    "    def _predict(self, input_convo):\n",
    "        gen_kwargs = {\n",
    "            'do_sample': True, 'temperature': 0.85, 'top_k': 30, 'top_p': 0.9, 'typical_p': 0.9, 'repetition_penalty': 1.2,\n",
    "            'max_length': self.max_length, 'stopping_criteria': self.stopping_criteria, 'pad_token_id': self.generator.tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        if isinstance(input_convo, str):\n",
    "            output = self.generator([\n",
    "                self.system_msg,\n",
    "                {\n",
    "                        'role': 'user',\n",
    "                        'content': input_convo\n",
    "                }\n",
    "                ],\n",
    "                **gen_kwargs\n",
    "            )\n",
    "            \n",
    "        elif isinstance(input_convo, list):\n",
    "            assert type(input_convo[0]) == dict, \"Conversation must be of type List[Dict]\"\n",
    "            \n",
    "            # transformed_convo = [{'role': k, 'content': v} for d in input_convo for k, v in d.items()]\n",
    "    \n",
    "            # final_convo = [self.system_msg] + transformed_convo\n",
    "            output = self.generator(input_convo, **gen_kwargs)\n",
    "\n",
    "        self.conversation = list(output)\n",
    "\n",
    "        return output[-1]['content']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:59:27.114830Z",
     "start_time": "2024-02-03T02:59:27.101920Z"
    }
   },
   "id": "7f931b1c34c3f921",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = ConversationChain(system_prompt=system_msg, generator=generator, stopping_criteria=stopping_criteria)\n",
    "\n",
    "test(\"My patient is presenting with persistent pain in their right knee (about a 7/10). They got this after squatting, but it appeared after they woke up and they said they didn't feel anything when they went to bed. They also mentioned that they have been experiencing some swelling and stiffness in the knee. They have been taking some over-the-counter pain medication, but it hasn't been helping much. They also mentioned that they have been experiencing some clicking and popping in the knee when they move it. They have no history of trauma to the knee, and they have not had any recent illnesses or fevers. They have no other significant medical history.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "146db280f2e4d10d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f53d4b17-ba8c-40cd-ad61-21233ac34aeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:59:29.072253Z",
     "start_time": "2024-02-03T02:59:29.045915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 26.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '''A 49 year(s) old female is here for consultation for joint pain and back pain and referred by PCP. Today patient complains of mild (pain level 3 /10), intermittent, waxing and waning pain in both hands, both hips and lower back for several years. States pain is worsened by activity, movement, sitting too long, standing too long and standing up from the chair, somewhat relieved by Tylenol Arthritis, Aleve PRN, Diclofenac bid (dose unrecalled at this time), heat and medication. Has morning stiffness lasts for about 2-3 hours. She denies any fever, chills, cough and shortness of breath.\n",
    "    She states she is experiencing chronic low back pain to which she had undergone PT with slight benefit (spinal manipulation) and Chiropractor without benefit. She notes undergoing X-ray of the L-Spine in 2007 ordered by her Chiropractor, which revealed Lumbar DDD. She reports minimal to moderate difficulty transitioning from sitting to standing and getting out of bed due to stiffness of the low back and b/l hips. At worst, her pain can be as high as 5-6/10. In 2022, she mentions being started on Diclofenac bid (dosage unrecalled) with slight benefit.\n",
    "    \n",
    "    I want you to give me a detailed analysis of the patient's information, followed by a comprehensive reasoning for your diagnosis and suggested next steps in treatment.''',\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': '''Patient Information:\n",
    "- Age: 49 years old\n",
    "- Gender: Female\n",
    "- Complaints: Mild, intermittent, waxing and waning pain in both hands, both hips and lower back for several years; chronic low back pain; minimal to moderate difficulty transitioning from sitting to standing and getting out of bed due to stiffness of the low back and bilateral hips\n",
    "\n",
    "Relevant History:\n",
    "- Pain worsens with activity, movement, sitting or standing for prolonged periods, and standing up from a seated position\n",
    "- Morning stiffness lasting for approximately 2-3 hours\n",
    "- Partial relief with over-the-counter medications such as acetaminophen and ibuprofen, prescription diclofenac, heat, and rest\n",
    "- Prior physical therapy yielded some improvement but was limited\n",
    "- Spinal manipulations through chiropractic care did not alleviate symptoms\n",
    "- Imaging studies in 2007 showed lumbar degenerative disc disease (L-Spine X-ray)\n",
    "- No reported fevers, chills, cough, or shortness of breath\n",
    "\n",
    "Potential Diagnoses:\n",
    "Based on the provided information, the following conditions could account for the patient's complaints:\n",
    "\n",
    "1. Osteoarthritis (OA): The most common form of arthritis affecting synovial joints characterized by cartilage degradation, osteophyte formation, subchondral bone sclerosis, and cyst formation leading to functional limitation and often accompanied by pain and inflammation. The distribution of painful sites—bilateral hands, hips, and lower back—supports an OA diagnosis, particularly given age and radiographic findings consistent with lumbar spondylosis. Furthermore, her response to nonsteroidal anti-inflammatory drugs (NSAIDs) supports the possibility of OA involvement. However, other forms of arthritis cannot be ruled out based solely on history and examination.\n",
    "\n",
    "Other Potential Conditions Considered:\n",
    "While there remains concern for alternative diagnoses, they appear less likely given the history and exam details. Nonetheless, additional possibilities warrant consideration, including rheumatologic causes like psoriatic arthritis, reactive arthritis, septic arthritis, polymyalgia rheumatica (PMR)/ giant cell arteritis (GCA), seronegative enthesopathy syndrome (SES), and crystal arthropathies such as gout and pseudogout. Less probable explanations may also involve neurological disorders mimicking musculoskeletal manifestations, although these scenarios seem substantially more remote.\n",
    "\n",
    "Recommended Next Steps in Treatment:\n",
    "1. Continue current NSAID regimen if tolerated, preferably prescribed doses rather than over-the-counter alternatives. Monitor side effects closely, especially considering gastric ulcers risk associated with long-term usage. Alternatively, consider switching to topical NSAIDs or capsaicin cream application locally onto affected areas to minimize systemic absorption. Additionally, explore adding muscle relaxants during flare-ups, e.g., cyclobenzaprine or tizanidine, to augment analgesia while minimizing adverse events compared to opioids.\n",
    "2. Encourage weight loss strategies, regular aerobic exercise programs, and flexibility training tailored to individual needs and abilities when feasible since improvements in mobility can reduce mechanical stress placed upon vulnerable articular structures implicated in pathogenesis. Consult physiatry specialists familiar with managing complex regional pain syndromes involving multiple peripheral joints concurrently for guidance on appropriate therapeutic exercises addressing specific impairments identified via focused assessment.\n",
    "3. Refer patients to orthopedics or sports medicine clinicians who specialize in regenerative treatments such as platelet-rich plasma (PRP) injections, viscosupplementation procedures, autologous stem cell therapies, etc., potentially offering substantial benefits beyond traditional conservative measures alone. Ensure proper screening tests have been conducted before proceeding further to rule out underlying bleeding disorders, coagulopathies, active malignancies, or infectious processes posing contraindications towards invasive intervention methods.\n",
    "4. Initiate collaborative discussions between all relevant providers involved within multifactorial management plans concerning shared goals, expectations, timelines, follow-up appointments, communication channels, available resources, educational materials, and support networks aimed at optimizing overall quality of life outcomes achievable across diverse domains impacted adversely by persistent discomfort.\n",
    "\n",
    "AI Generated Advice Disclaimer:\n",
    "The above recommendations were generated by an artificial intelligence model trained to identify various medical conditions and suggest possible courses of action.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "test = ConversationChain(system_prompt=system_msg, conversation=conversation, generator=generator, stopping_criteria=stopping_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 16.9 s, total: 2min 21s\n",
      "Wall time: 2min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Thank you for providing updated information regarding the patient\\'s past dermatological condition. Based on the new input, we must reassess our previous conclusions and consider whether the original hypothesis of osteoarthritis sufficiently explains all symptoms described. With particular attention drawn to the report of psoriasis affecting the scalp, face, eyebrows, behind the ears, inside the ears, and bilateral legs, an expanded perspective is required. Specifically, it raises concerns for a broader category of autoimmune illnesses known collectively as Spondyloarthritides (SpAs), which comprise a heterogeneous group of disorders sharing overlapping genetic susceptibility factors and frequently exhibiting cutaneous and articular abnormalities alike. These entities encompass predominantly axial presentations classified under Axial SpA and primarily peripheral patterns categorized as Peripheral SpA, whereas others fall into mixed phenotype categories. Within this contextual framework, we offer revised insights below after incorporating the recent revelation of preexisting psoriasis.\\n\\nUpdated Analysis & Hypotheses:\\nUpon integrating the knowledge about the patient\\'s pre-existing psoriasis, another plausible explanation warrants exploration - namely, Psoriatic Arthritis (PsA), one type of Spondylarthritis (SpA). Approximately 8% to 30% of individuals afflicted with psoriasis develop PsA, commonly featuring skin lesions before articular signs emerge. Notably, nearly half of those affected exhibit exclusive peripheral arthritis, as observed in our case, whereby the patient has experienced swelling, tenderness, and reduced range motion predominantly in small diarthroses located distally along limbs (hands and hip joints). Moreover, the description of \"morning stiffness\" enduring two to three hours resonates with certain aspects of established diagnostic criteria used for PsA classification systems, notably the ClASsification Criteria for Psoriatic ARThritis (CASPAR) tool devised specifically for differentiating PsA cases amidst the general population presenting with undifferentiated inflammatory arthritis. Despite the absence of sacroiliac joint involvement or inflammatory bowel disease mentioned in the initial presentation, the presence of psoriasis coupled with polyarticular peripheral arthritis necessitates heightened vigilance against possible progression toward a definitive PsA diagnosis, thus mandating careful monitoring and evaluation.\\n\\nRecommended Next Steps in Management:\\n1. Assess the need for laboratory investigations evaluating markers indicative of inflammation (e.g., ESR, CRP) to establish baseline values and detect fluctuations corresponding to changes in disease status. Although their utility might be hindered by poor sensitivity and specificity levels, periodic measurements serve as valuable adjunct tools reflective of global health trends and aid in tracking responsiveness to instituted treatments.\\n2. Introduce targeted pharmaceutical interventions according to the severity and extent of the patient\\'s joint symptoms. For instance, conventional synthetic disease-modifying antirheumatic drugs (csDMARDs) such as methotrexate, sulfasalazine, or leflunomide can mitigate inflammation and preserve structural integrity. Likewise, biologic DMARDs (bDMARDs), specifically tumor necrosis factor inhibitors (TNFIs), interleukin-17 (IL-17) inhibitors, or IL-12/23 inhibitors, represent efficacious alternatives for recalcitrant instances requiring potent immunomodulation alongside csDMARDs.\\n3. Persistently advocate for healthy lifestyle choices despite challenges imposed by chronic pain and fatigue. Regular engagement in low-impact activities proven beneficial in ameliorating pain, enhancing functionality, and bolstering psychological resilience includes swimming, yoga, tai chi, aquatic exercises, walking, cycling, and meditation practices. Exercise routines should prioritize improving strength, flexibility, posture, balance, and coordination, fostering robust self-efficacy beliefs and promoting greater independence.\\n4. Foster ongoing dialogue among healthcare teams responsible for delivering integrated care, thereby harmonizing complementary approaches, reducing redundancy, and maximizing cost efficiency. By encouraging multiparametric collaboration, healthcare providers can effectively coordinate interdisciplinary efforts dedicated to achieving optimal patient results across distinct facets influenced negatively by pervasive pain and disability.\\n\\nRevised AI Generated Advice Disclaimer:\\nThis section contains recommendations derived from an artificial intelligence algorithm programmed to recognize varying medical conditions and propose suitable courses of action. Please remember that these suggestions ought to supplement, not replace, expert judgment rendered by qualified healthcare professionals.<|im_end|>'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print(test('''The patient reported being diagnosed with Psoriasis on scalp, face, eyebrows, behind the ears, inside the ears, and b/l LE in 2004 by her Dermatologist treated by topical medications (cream and ointment). Currently, she only notes of having a Plaque at the base of her hairline.'''))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T03:01:54.713924Z",
     "start_time": "2024-02-03T02:59:32.189044Z"
    }
   },
   "id": "7f2b5a4dc12ece16",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5d1c8-234a-4d49-b005-11dbbbfb4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_in = '''History: A 55-year-old male presents with a 2-month history of progressive fatigue, intermittent low-grade fever, and a 10-pound weight loss. He mentions a persistent, dry cough and mild shortness of breath. The patient has a 20-year history of smoking but quit 5 years ago. He denies any recent travels or sick contacts. He works as a carpenter.\n",
    "\n",
    "Physical Examination: The patient appears mildly distressed. Vital signs are normal except for a mild fever. Lung auscultation reveals faint crackles at the base of the right lung. There's no lymphadenopathy or clubbing. Skin examination is unremarkable.\n",
    "\n",
    "Lab Findings:\n",
    "\n",
    "CBC: Mild anemia.\n",
    "Chest X-Ray: A small, ill-defined opacity in the right lower lobe.\n",
    "Spirometry: Mild restrictive pattern.\n",
    "Sputum Culture: No growth.\n",
    "Tuberculin Skin Test (TST): Negative.\n",
    "Serum Electrolytes, Renal and Liver Function Tests: Normal.\n",
    "Differential Diagnosis:\n",
    "\n",
    "Lung Cancer: Given the history of smoking and lung opacity.\n",
    "Tuberculosis (TB): Considering the chronic cough and weight loss, despite the negative TST.\n",
    "Sarcoidosis: Possible, given the lung findings and restrictive lung disease pattern, but no lymphadenopathy or skin findings typical of sarcoidosis are present.\n",
    "Hypersensitivity Pneumonitis: Occupational exposure could be a risk factor, but the presentation is atypical.\n",
    "Further Investigations:\n",
    "\n",
    "CT scan of the chest for a better characterization of the lung lesion.\n",
    "Bronchoscopy with biopsy if indicated by CT findings.\n",
    "Serum ACE levels and possibly a gallium scan to evaluate for sarcoidosis.\n",
    "Detailed occupational history to assess for hypersensitivity pneumonitis risk.\n",
    "\n",
    "Give me a detailed analysis of this patient's information, and suggest some potential diagnoses, next steps, and future treatment options.\n",
    "'''\n",
    "\n",
    "test = ConversationChain(system_prompt=system_msg, generator=generator, stopping_criteria=stopping_criteria)\n",
    "\n",
    "test(user_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2995c93-77d5-4ce7-98fc-3c43c851d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationChain:\n",
    "    def __init__(self, system_prompt='', conversation=[], generator=None, stopping_criteria=[], max_length=32_000):\n",
    "        self.system_msg = {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        }\n",
    "        \n",
    "        self.conversation = [system_msg] + conversation\n",
    "        \n",
    "        assert generator is not None, \"Generator (pipeline) is required!\"\n",
    "        self.generator = generator\n",
    "        \n",
    "        self.stopping_criteria = stopping_criteria\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, message):\n",
    "        new_conversation = self.conversation + [{\n",
    "            'role': 'user',\n",
    "            'content': message\n",
    "        }]\n",
    "\n",
    "        return self._predict(new_conversation)\n",
    "\n",
    "    def _predict(self, input_convo):\n",
    "        gen_kwargs = {\n",
    "            'do_sample': True, 'temperature': 0.85, 'top_k': 30, 'top_p': 0.9, 'typical_p': 0.9, 'repetition_penalty': 1.2,\n",
    "            'max_length': self.max_length, 'stopping_criteria': self.stopping_criteria, 'pad_token_id': self.generator.tokenizer.eos_token_id\n",
    "        }\n",
    "        \n",
    "        if isinstance(input_convo, str):\n",
    "            output = self.generator([\n",
    "                self.system_msg,\n",
    "                {\n",
    "                        'role': 'user',\n",
    "                        'content': input_convo\n",
    "                }\n",
    "                ],\n",
    "                **gen_kwargs\n",
    "            )\n",
    "            \n",
    "        elif isinstance(input_convo, list):\n",
    "            assert type(input_convo[0]) == dict, \"Conversation must be of type List[Dict]\"\n",
    "            \n",
    "            # transformed_convo = [{'role': k, 'content': v} for d in input_convo for k, v in d.items()]\n",
    "    \n",
    "            # final_convo = [self.system_msg] + transformed_convo\n",
    "            output = self.generator(input_convo, **gen_kwargs)\n",
    "\n",
    "        self.conversation = list(output)\n",
    "\n",
    "        return output[-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75eac9-22dd-4f8b-bec0-a6e51586e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "project = \"aidx-finetune\"\n",
    "base_model_name = \"mixtral\"\n",
    "run_name = (\n",
    "        base_model_name\n",
    "        + \"-\"\n",
    "        + project\n",
    ")\n",
    "output_dir = \"./training/\" + run_name\n",
    "cpu_count = os.cpu_count()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    warmup_steps=10,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=2,\n",
    "    dataloader_num_workers=cpu_count,\n",
    "    learning_rate=0.0002,\n",
    "    lr_scheduler_type='cosine',\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5,  # Save checkpoints every 5 steps\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    report_to=\"wandb\",  # Comment this out if you don't want to use weights & baises\n",
    "    run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",  # Name of the W&B run (optional)\n",
    "    gradient_checkpointing_kwargs={\n",
    "        'use_reentrant': False\n",
    "    },\n",
    "    # deepspeed=\"/workspace/zero3.json\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e66af-4c3a-43f0-b314-61ce06e14dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"gate\",\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"w1\",\n",
    "            \"w2\",\n",
    "            \"w3\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8cad2-02b6-47b4-b630-4a249e8eadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = create_data_collator(base_model_id)\n",
    "compute_bertscore = load_bertscore()\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        # train_dataset=train_data,\n",
    "        # eval_dataset=val_data,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_bertscore,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306bd34-cff9-41d5-9bbc-df8cc15ee9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9c36c-3de0-432c-90f1-36e7a22282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parquet_dir = '/workspace/data/parquet'\n",
    "    stream_data = False\n",
    "    base_model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "    \n",
    "    cpu_count = os.cpu_count()\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    \n",
    "    print(\"Setting up training environment...\")\n",
    "    accelerator = setup_training_env()\n",
    "    data_collator = create_data_collator(base_model_id)\n",
    "    accelerator.print(\"Done setting up training environment...\")\n",
    "    \n",
    "    accelerator.print(f\"Number of CPUs available: {cpu_count}\")\n",
    "    accelerator.print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    \n",
    "    accelerator.print(\"Loading data...\")\n",
    "    train_data, val_data, train_count, val_count = load_data(parquet_dir, stream=stream_data, cpu_count=cpu_count, accelerator=accelerator)\n",
    "    accelerator.print(\"Done loading data...\")\n",
    "    \n",
    "    accelerator.print(\"Loading compute_bertscore...\")\n",
    "    compute_bertscore = load_bertscore()\n",
    "    accelerator.print(\"Done loading compute_bertscore...\")\n",
    "\n",
    "    max_batch_size = 1\n",
    "    num_epochs = 1\n",
    "\n",
    "    # x = train_data.select(range(100))\n",
    "    \n",
    "    # def test(a):\n",
    "    #     return {\n",
    "    #         'attention_mask': a['attention_mask'][:512],\n",
    "    #         'input_ids': a['input_ids'][:512]\n",
    "    #     }\n",
    "    \n",
    "    # x = x.map(test)\n",
    "    \n",
    "    accelerator.print(\"Loading model trainer...\")\n",
    "    trainer = load_model_trainer(\n",
    "            base_model_id,\n",
    "            compute_bertscore,\n",
    "            data_collator,\n",
    "            train_data,\n",
    "            val_data,\n",
    "            max_batch_size,\n",
    "            num_epochs,\n",
    "            cpu_count//gpu_count,\n",
    "            accelerator\n",
    "        )\n",
    "    \n",
    "    print_gpu_utilization(accelerator)\n",
    "\n",
    "    # train_result = trainer.train()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98921e84-478f-4fe5-b3e3-df32446e9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(main, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976931ee-c79a-4283-b723-9f3c1086915a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
