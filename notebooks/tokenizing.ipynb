{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import os\n",
    "\n",
    "HOST_IP = os.environ['DATABASE_IP']\n",
    "DATABASE_USER = os.environ['DATABASE_USER']\n",
    "DATABASE_PASSWORD = os.environ['DATABASE_PASSWORD']\n",
    "DATABASE_PORT = os.environ['DATABASE_PORT']\n",
    "\n",
    "connection_url = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=DATABASE_USER,\n",
    "    password=DATABASE_PASSWORD,\n",
    "    host=HOST_IP,\n",
    "    port=DATABASE_PORT,\n",
    "    database=\"mimicllm\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_url)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7884803769fce78b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae8cb5d30a98e99",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "def generate_prompt(system, input, output, separate=False):\n",
    "    # convert to instruction formatting\n",
    "    input_prompt = f\"<|im_start>system\\n{system}\\n<|im_end|>\\n<|im_start|>user\\n{input}\\n<|im_end|><|im_start|>assistant\\n\"\n",
    "    output_prompt = f\"{output}\\n<|im_end|></s>\"\n",
    "    \n",
    "    if separate:\n",
    "        return {\n",
    "            \"input\": input_prompt,\n",
    "            \"output\": output_prompt\n",
    "        }\n",
    "    \n",
    "    return input_prompt + output_prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ea6aae38049bcf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "def format_batch_query(batch, start_at=0):\n",
    "    return sql_text(f\"\"\"\n",
    "    SELECT sample_id, input, output\n",
    "    FROM mimicllm.data\n",
    "    ORDER BY sample_id ASC\n",
    "    LIMIT {batch}\n",
    "    OFFSET {start_at}\n",
    "    \"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbcfe81ee6d190c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "def get_batch(batch_size, start_at=0):\n",
    "    query = format_batch_query(batch_size, start_at)\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "\n",
    "def generate_batches(batch_size):\n",
    "    query = sql_text(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM mimicllm.data\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    total = df.iloc[0]['count']\n",
    "    for i in range(0, total, batch_size):\n",
    "        yield get_batch(batch_size, i)\n",
    "        \n",
    "def get_data(batch_size):\n",
    "    # get the total number of samples\n",
    "    query = sql_text(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM mimicllm.data\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    total = df.iloc[0]['count']\n",
    "    \n",
    "    return generate_batches(batch_size), ceil(total/batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "633c5b28adeba9bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_all_sample_ids():\n",
    "    query = sql_text(\"\"\"\n",
    "    SELECT sample_id\n",
    "    FROM mimicllm.data\n",
    "    ORDER BY sample_id ASC\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df['sample_id'].tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c8ecf53ff9ad2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_base_id(sample_id):\n",
    "    \"\"\"Extract the base ID from the sample ID.\"\"\"\n",
    "    if sample_id.endswith(\"discharge\"):\n",
    "        return sample_id.replace(\"_discharge\", \"\")\n",
    "    return '_'.join(sample_id.split('_')[:-1])\n",
    "\n",
    "def extract_numeric_id(sample_id):\n",
    "    \"\"\"Extract the numeric part of the sample ID.\"\"\"\n",
    "    parts = sample_id.split('_')\n",
    "    if parts[-1].isdigit():\n",
    "        return int(parts[-1])\n",
    "    return None  # For 'discharge' or other non-numeric parts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4afba8753e75a53b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def batch_strings(string_list, batch_size):\n",
    "    # Initialize the list of batches and the current batch\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "\n",
    "    # Iterate over each string in the list\n",
    "    for string in string_list:\n",
    "        # Add string to the current batch\n",
    "        current_batch.append(string)\n",
    "\n",
    "        # If the current batch reaches the batch size, add it to the batches list\n",
    "        if len(current_batch) == batch_size:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []  # Start a new batch\n",
    "\n",
    "    # Add the last batch if it contains any strings\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4dbb1a11b01bd7f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def upload_to_db(df, mimicllm_engine, table=\"data\"):\n",
    "    df.to_sql(\n",
    "        table,\n",
    "        mimicllm_engine,\n",
    "        schema=\"mimicllm\",\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        method=\"multi\",\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80652efd7a34ed41"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_ids = get_all_sample_ids()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e38695be15cad3d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "system_prompt = \"\"\n",
    "# BATCH_SIZE = 540_000\n",
    "BATCH_SIZE = 1\n",
    "MAX_LENGTH = 32_000\n",
    "\n",
    "# organize sample_ids into batches\n",
    "batched_sample_ids = batch_strings(sample_ids, BATCH_SIZE)\n",
    "batch_iterator, total_batches = get_data(BATCH_SIZE)\n",
    "\n",
    "outer_bar = tqdm(total=total_batches, desc=\"Batches\", leave=True)\n",
    "inner_bar = tqdm(total=BATCH_SIZE, desc=\"Samples\", leave=False)\n",
    "\n",
    "last_skipped_id = {}\n",
    "tokenized_prompts = []\n",
    "\n",
    "for i, batch in enumerate(batch_iterator):\n",
    "    inner_bar.reset()\n",
    "    for index, row in batch.iterrows():\n",
    "        base_id = extract_base_id(row['sample_id'])\n",
    "        numeric_id = extract_numeric_id(row['sample_id'])\n",
    "        inner_bar.set_description(f\"Samples - {row['sample_id']}\")\n",
    "\n",
    "        # Skip logic for non-discharge samples\n",
    "        if base_id in last_skipped_id and numeric_id is not None:\n",
    "            if numeric_id >= last_skipped_id[base_id] and not row['sample_id'].endswith(\"discharge\"):\n",
    "                continue\n",
    "        \n",
    "        prompt = generate_prompt(system_prompt, row['input'], row['output'])\n",
    "        tokenized = tokenize(prompt)\n",
    "        \n",
    "        inner_bar.set_postfix_str(f\"Length: {len(tokenized['input_ids'][0])}\")\n",
    "        \n",
    "        if len(tokenized['input_ids'][0]) > MAX_LENGTH:\n",
    "            outer_bar.write(f\"Batch {i}: Sample {row['sample_id']} is too long, rest were skipped\")\n",
    "            if numeric_id is not None:\n",
    "                last_skipped_id[base_id] = numeric_id\n",
    "        else:\n",
    "            tokenized_prompts.append(tokenized)\n",
    "            \n",
    "        inner_bar.update(1)    \n",
    "    \n",
    "    outer_bar.update(1)\n",
    "    \n",
    "serialized_tokens = pd.DataFrame(tokenized_prompts).map(lambda x: pickle.dumps(x))\n",
    "upload_to_db(serialized_tokens, engine, table=\"tokenized_data\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5f39bb8ee7a45b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batch_sizes = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]  # Example batch sizes\n",
    "query_times = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    start_time = time.time()\n",
    "    _ = get_batch(batch_size)  # Assuming get_batch is your function to fetch data\n",
    "    end_time = time.time()\n",
    "    query_times.append(end_time - start_time)\n",
    "    \n",
    "query_times"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de2b366495819f27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "memory_usages = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    batch_data = get_batch(batch_size)\n",
    "    memory_usage = sys.getsizeof(batch_data)  # This gives an estimate in bytes\n",
    "    memory_usages.append(memory_usage)\n",
    "\n",
    "memory_usages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbba06aa2fad31ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, query_times, label='Query Time')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batch_sizes, memory_usages, label='Memory Usage')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Memory Usage (bytes)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8941f37ffba6668",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def best_fit(X, Y):\n",
    "\n",
    "    xbar = sum(X)/len(X)\n",
    "    ybar = sum(Y)/len(Y)\n",
    "    n = len(X) # or len(Y)\n",
    "\n",
    "    numer = sum([xi*yi for xi,yi in zip(X, Y)]) - n * xbar * ybar\n",
    "    denum = sum([xi**2 for xi in X]) - n * xbar**2\n",
    "\n",
    "    b = numer / denum\n",
    "    a = ybar - b * xbar\n",
    "\n",
    "    print('best fit line:\\ny = {:.2f} + {:.2f}x'.format(a, b))\n",
    "\n",
    "    return a, b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "889d88ebb4bbb8da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "time_a, time_b = best_fit(batch_sizes, query_times)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fefee83773bd07b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mem_a, mem_b = best_fit(batch_sizes, memory_usages)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e69a881d496ec1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate the batch size for 32gb\n",
    "# 32gb = mem_a + mem_b * batch_size\n",
    "# 32gb - mem_a = mem_b * batch_size\n",
    "# (32gb - mem_a) / mem_b = batch_size\n",
    "(32_000_000_000 - mem_a) / mem_b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bac80e0cf65512f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate the time for 540_000 batch size\n",
    "# time_a + time_b * batch_size = time\n",
    "time_a + time_b * 540_000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b8dac5c1c1027c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4315df583dd09d48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
